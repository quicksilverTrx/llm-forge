# llm-forge


## Core Transformer Components Implementation

### Current Implementation Status from Scratch.
-  Basic dot-product attention
-  Scaled attention mechanics
-  Multi Head Attention
-  Rotary Positional Encodings
-  Grouped Query Attention
-  Flash Attention (High Level conceptual)
-  GPT-2 Small Implementation with LM head 
